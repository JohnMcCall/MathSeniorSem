\documentclass{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{algorithm2e}

\title{Error Correcting Codes}
\author{John McCall\\Adviser: Peter Dolan}

\begin{document}
\RestyleAlgo{boxruled}

\maketitle

\section{Abstract}
[Coming Soon]

\section{Introduction}
Communication plays a huge role in today's society. Electronic forms of communication become more prevalent every year. Whether it is listening to a CD in the car, reading an article on the internet, or talking with a friend via a cell phone, electronic communication impacts our everyday lives. However, these forms of communication are not always perfect. For instance, that CD could get scratched and some of its data will be lost. Error correcting codes are a way of coping with this loss. 

The idea behind error correcting codes is to encode and transmit the data with a sufficient amount of redundancy to be able to reconstruct any data that may get lost. This is why even if that CD gets some minor scratches it will still be able to play perfectly. Again, these codes are not perfect. If that scratch is too large, some of the data will be irrecoverable and there will be a noticeable effect on the music playing.

In section 1 will provide the necessary abstract algebra background to understand the remainder of the paper. Section 2 will introduce coding theory and give examples of some codes. In section 3 we will go into detail about Generalized Reed-Solomon codes and will walk through an example.

\section{Background}
This section will provide all the necessary background in abstract algebra that is needed to understand Generalized Reed-Solomon codes. Specifically it will define groups, cyclic groups, rings and fields. Introduce the finite field GF(2) as well as the extension field GF($2^{m}$). Then it will describe how addition and multiplication works in this field.

\subsection{Groups, Rings, and Fields}

\subsection{Addition and Multiplication in GF($2^{m}$)}

\section{Codes}
A \textit{code} is a rule for converting information (usually a letter or word) into another representation. Oftentimes, this other representation is of a different form. \textit{Encoding} is the actual act of conversion. A simple code would be to map each letter of the alphabet to a number. For instance, ``A" maps to ``1", ``B" maps to ``2", etc. In this example, the letters are our \textit{source alphabet}, and the numbers are our \textit{target alphabet}. To encode the word ``ALGEBRA" we would simply replace all the letters with the appropriate numbers. In this case, ``112752171" is the result. The act of \textit{decoding} reverses the code and returns the word to its original state.

Codes are very useful when storing data, or transmitting it over long distances, as they provide an easy, consistent, way of representing data. However, this data may be corrupted in transmission. \textit{Random errors} are a type of error that corrupts individual symbols during transmissions. Corruption, in this context, means changing the symbol to another symbol in our alphabet. Going back to our example, if two random errors occur during transmission, our word may arrive as: ``117712171". A ``2" was replaced with a ``7" and a ``5" was replaced with a ``1". As a result our decoded word is: ``ARGABRA". A \textit{burst error} is when a large chunk of symbols in a row become corrupted. These can be particularly troublesome as they can corrupt a large portion of the data, and are common in several types of communication channels.

Codes that can detect these error are called \textit{error detecting} codes. Codes that can correct these errors are called \textit{error correcting} codes. To reiterate, there is no perfect code which can correct or detect all errors. The ability to correct even a small amount of errors is better than the alternative of having to retransmit the data every time an error occurs.

\subsection{Cyclic Redundancy Check}
An example of an actual code used in the field is a \textit{cyclic redundancy check (CRC)}. A CRC is a type of error detecting code used to detect accidental changes in data. To detect an error, a fixed length \textit{check value} is appended to the end of the message. The check value is based on the remainder of a polynomial division of the messages contents. When the message is received the check value is recomputed and the appropriate measures to fix the corruption can be taken if the two values do not match.

CRCs are commonly used because they are easy to implement in binary hardware, easy to analyze, and are quite good at detecting burst errors. Usually, an $n$-bit CRC used on an arbitrary length message will detect any single burst error less than or equal to $n$ bits and will detect a fraction $1 - 2^{-n}$ of all longer burst errors. 

The specification of a CRC code requires a \textit{generator polynomial}. This polynomial is the divisor in polynomial long division, the dividend is the message, the quotient is thrown away, and the remainder becomes the result. It is important to note that the coefficients of the polynomial are computed using the carry-less arithmetic of a 
finite field. Most CRCs use the finite field GF(2) in practice as this field only has two elements, 0 and 1, which meshes nicely with the architecture of computers.

A CRC is called an $n$-bit CRC when its check value has $n$ bits. Any $n$ has multiple CRCs, each with a different polynomial. Any such polynomial has highest degree $n$, which means that it has $n + 1$ terms, and that its encoding requires $n + 1$ bits. A CRC and associated polynomial are given a name of the form CRC-$n$-XXX. The simplest CRC is CRC-1 and it uses the generator polynomial $x + 1$. This is also called a parity bit, which is the simplest form of error detection.

\subsubsection{Application and Integrity}
A device that implements a CRC calculates the check value for each data block to be sent or stored and appends it to the end of the block. This forms a \textit{codeword}. When a codeword is read or received the device does one of two things. It either recalculates the check value of the data block and compares it to the check value from the codeword, or it performs a CRC on the entire codeword and compares the resulting check value with an expected \textit{residue} constant. If the check values do not match the block contains some errors in its data and the device may take corrective actions. However, because of the nature of error-checking, it is possible that some errors may remain undetected, even if the check values match. 

CRCs are designed to protect accidental errors incurred from communication channels, and can give assurance of the integrity of delivered messages. CRCs are not suitable when it comes to protecting against intentional modification of data for several reasons. There is no authentication so an attacker can edit the message and recompute the CRC without being detected. CRC is easily reversible which makes it unsuitable for use in digital signatures. Finally, CRC is a linear function with a property that $crc(x \oplus y) = crc(x) \oplus crc(y)$. Because of this, even if the CRC is encrypted with a stream cipher, the message and its CRC can be altered without knowledge of the encryption key. 

\subsubsection{Computing a CRC}
To compute an $n$-bit binary CRC, start with the message encoded in binary:
\begin{verbatim}
11010011101100
\end{verbatim}
This is padded with $n$ zeroes, corresponding to the bit length $n$ of the CRC. The first calculation for a 3-bit CRC is as follows:
\begin{verbatim}
11010011101100 000 <--- input padded with 3 bits
1011               <--- divisor (4 bits) 
------------------
01100011101100 000
\end{verbatim}
If the input bit above the leftmost divisor bit is 0, do nothing. If the bit is 1, the divisor is XORed with the input. The divisor is then shifted one bit to the right. This process is repeated until the divisor reaches the right most bit of the input. The calculation in its entirety is:
\begin{verbatim}
11010011101100 000 <--- input padded with 3 bits
1011               <--- divisor
01100011101100 000 <--- result
 1011              <--- divisor ...
00111011101100 000
  1011
00010111101100 000
   1011
00000001101100 000
       1011
00000000110100 000
        1011
00000000011000 000
         1011
00000000001110 000
          1011
00000000000101 000
           101 1
------------------
00000000000000 100 <--- remainder (3 bits)
\end{verbatim}

In this case the leftmost bit in the divisor zeroes out all the bits in the input so all that's left is the remainder. To check the validity of the received message simply perform the computation again, but instead of padding the input with zeroes, add the check value instead. If there are no errors the remainder after this computation should be zero.
\begin{verbatim}
11010011101100 100 <--- input with check value
1011               <--- divisor
01100011101100 100 <--- result
 1011              <--- divisor ...
00111011101100 100

......
  
00000000001110 100
          1011
00000000000101 100 
           101 1
------------------
                 0 <--- remainder
\end{verbatim}

\subsection{Linear Codes}
A \textit{linear code} of length $n$ and rank $k$ is a linear subspace $C$ with dimension $K$ of the vector space $\mathbb{F}^{n}_{q}$ where $\mathbb{F}_{q}$ is the finite field with $q$ elements. For our purposes $q = 2$. The vectors in $C$ are called \textit{codewords}. The \textit{weight} of a codeword is the number of its nonzero elements. The \textit{distance} between two codewords is the number of elements in which they differ. This is called the \textit{Hamming distance}.

Since $C$ is a linear subspace of $\mathbb{F}^{n}_{q}$, $C$ can be represented by a basis. A martix can be constructed from this basis where each row in the matrix is an element from the basis set. This martix is called the \textit{generator matrix} of $C$.

The \textit{dual code} of a code $C$, denoted $C^{\perp}$ is the code
\begin{center}
$C^{\perp} = \{\textbf{x} \in F^{n} \mid \textbf{x} \cdot \textbf{c} = 0, \text{ for all } c \in C\}$
\end{center}
where $\textbf{x} \cdot \textbf{c}$ is the usual dot product. If $C$ is a linear code, then $C^{\perp \perp} = C$. Dual codes are useful because they possess the property that if $G$ is a generator matrix for $C$ then $\textbf{x}$ is in $C$ if and only if $G\textbf{x}^{\mathsf{T}} = \textbf{0}$.

The generator matrix $H$ for the dual code $C^{\perp}$ of linear code $C$ is called a \textit{check matrix} for $C$. Since $C^{\perp \perp} = C$, we can use the check matrix $H$ for $C$ to define $C$ as:
\begin{center}
$C = \{\textbf{x} \mid H\textbf{x}^{\mathsf{T}} = \textbf{0}\}$.
\end{center}
Often a code is defined using a check matrix. [Insert example here?]

\section{Generalized Reed-Solomon Codes}
Reed-Solomon codes were first introduced in 1961 by I.S. Reed and G. Solomon. These codes are somewhat rare in that they have both useful practical applications and an interesting mathematical foundation. Today, Reed-Solomon codes are used in many applications such as CD players and deep-space communications.

Reed-Solomon codes are a linear code. An $n$ length and $k$ dimension RS code is capable of correcting $\left \lfloor{\dfrac{n - k}{2}}\right \rfloor$ symbol errors. Because of this RS codes are quite adept at correcting burst errors. For instance, lets say we have an RS code of length $n = 128$ and dimension $k = 120$. Each symbol of our code will be made up of eight bits. Since $n - k = 128 - 120 = 8$ this means that we can correct any four symbol errors in our block of 128 symbols. This is true even if they are four consecutive symbols, as could happen due to a burst error.

J. Hall presents, in \textit{Notes on Coding Theory}, generalized Reed-Solomon Codes. It is these generalized codes that we will be focusing on for the remainder of this paper. Below we will discuss the basic structure of these codes. After that we will go into detail about how to encode and decode a generalized Reed-Solomon Code.

[There will also be an example to go along with this section in the future, unfortunately it is incomplete in this version] 

\subsection{The Basics}
Let $F$ be a field. Choose nonzero elements $v_{1},..., v_{n} \in F$ and distinct elements $\alpha_{1},...,\alpha_{n} \in F$. Set $\textbf{v} = (v_{1},..., v_{n})$ and $\boldsymbol\alpha = (\alpha_{1},...,\alpha_{n})$ For $0 \le k \le n$ \textit{generalized Reed-Solomon codes} are defined as:
\begin{center}
$\text{GRS}_{n,k}(\boldsymbol\alpha, \textbf{v}) = \{(v_{1}f(\alpha_{1}),v_{2}f(\alpha_{2}),...,v_{n}f(\alpha_{n}))\;\mid\;f(x) \in F[x]_{k}\}$.
\end{center}
Here $F[x]_{k}$ is the set of polynomials in $F[x]$ with degree less than $k$. If $f(x)$ is a polynomial, then $\textbf{f}$ is its associated codeword. This codeword is also dependent on $\boldsymbol\alpha \text{ and } \textbf{v}$. We can write

\begin{center}
$\textbf{ev}_{\boldsymbol\alpha,\textbf{v}}(f(x)) = (v_{1}f(\alpha_{1}),v_{2}f(\alpha_{2}),...,v_{n}f(\alpha_{n}))$,
\end{center}
where $f = \textbf{ev}_{\boldsymbol\alpha,\textbf{v}}(f(x))$ when the polynomial $f(x)$ is evaluated at $\boldsymbol\alpha$ and scaled by $\textbf{v}$.

The distance between two codewords is defined as the number of symbols in which the sequences differ, in other words it is the Hamming distance. The minimum distance between two codewords for GRS codes is

\begin{center}
$\text{d}_{\text{min}} = n - k + 1$.
\end{center}

A key concept is that any codeword which has up to $k$ entries equal to 0 corresponds to a polynomial of degree less than $k$ whose values matching the 0 polynomial in $k$ points must be the 0 polynomial. This is true since any polynomial of degree less than $k$ is uniquely determined by its values at $k$ distinct points. Which means that for any $n$-tuple $\textbf{f}$, we can reconstruct the polynomial $f(x)$ of degree less than $n$ such that $\textbf{f} = \textbf{ev}_{\boldsymbol\alpha,\textbf{v}}(f(x))$. Let

\begin{center}
$L(x) = \prod\limits_{i=1}^{n} (x - \alpha_{i})$
\end{center}
and
\begin{center}
$L_{i}(x) = L(x)/(x - \alpha_{i}) = \prod\limits_{j \neq i} (x - \alpha_{j})$.
\end{center}
Both $L(x)$ and $L_{i}(x)$ are \textit{monic} polynomials of degrees $n$ and $n - 1$, respectively. A polynomial is monic if the leading coefficient is equal to 1. Since the $i^{\text{th}}$ coordinate of vector $\textbf{f}$ is $v_{i}f(\alpha_{i})$ we can use Lagrange interpolation [insert reference to Lagrange interpolation here] to calculate
\begin{center}
$f(x) = \sum\limits_{i=1}^{n} \dfrac{L_{i}(x)}{L_{i}(\alpha_{i})}f(\alpha_{i})$.
\end{center}

The polynomial $f(x)$ has degree less than $k$, while the interpolation polynomial of the righthand side has degree of $n - 1$. The solution to this problem allows us to calculate the dual of a GRS code more easily. Hall gives a theorem in Chapter 5 of \textit{Notes on Coding Theory} stating that:
\begin{center}
$\text{GRS}_{n,k}(\boldsymbol\alpha, \textbf{v})^{\perp} = \text{GRS}_{n,n - k}(\boldsymbol\alpha, \textbf{u})$,
\end{center}
where $\textbf{u} = (u_{1},...,u_{n}) \text{ with } u_{i}^{-1} = v_{i}\prod\limits_{j \neq i} (\alpha_{i} - \alpha_{j})$.

To verify that $\textbf{f}$ is a codeword in $C = \text{GRS}_{n,k}(\boldsymbol\alpha, \textbf{v})$ it is not necessary to compare it to every $\textbf{g} \text{ of } C^{\perp} = \text{GRS}_{n,n-k}(\boldsymbol\alpha, \textbf{v})$. Instead, we can use a basis of $C^{\perp}$, which is also a check matrix for $C$. Using a check matrix to define a linear code has its benefits. One such benefit is that it will allow us to use \textit{syndrome decoding}, which will be discussed in more detail in a later section.

\subsection{Encoding GRS Codes}
The field we are using to encode is GF($2^{6}$). This is the field of all 6 bit binary strings. Addition and multiplication work as described above in this field. This gives us 64 different symbols we can use in our source alphabet. For our example we chose to map 64 ASCII characters to the elements in GF($2^{6}$). Specifically, we mapped characters 32-95 inclusive. Because this map exists, we do not have to think directly about the binary strings underneath. We can write the elements of GF($2^{6}$) as those characters as well as perform the field operations on those characters. A Ruby script was used to generate an addition and multiplication table for these operations. The tables as well as the script can be seen in appendix A. 

A GRS code is composed of two parts. The first part is the actual message that we are sending. The message is $n$ symbols long and can be written as an $n - 1$ degree polynomial. The second part is the parity symbols. There are $r = n - k$ parity symbols appended to the end of the message to be used to ensure correctness after transmission. 

In order to find the parity symbols we need to use a generating polynomial. The generating polynomial for a Reed-Solomon code is of the following form:
\begin{center}
$g(x) = g_{0} + g_{1}x + g_{2}x^{2} + ... + g_{r - 1}x^{r - 1} + x^{r}$
\end{center}
In order to calculate $g(x)$ we need to find an element, $a$ in our group which is a primitive element. Then,
\begin{center}
$g(x) = (x - a)(x - a^{2})...(x - a^{r})$
\end{center}

Once we've found $g(x)$ we can divide the original message polynomial by $g(x)$. The remainder of this division is the parity symbols which we then add to the end of the message. Now that the message is complete we can send it.

\subsection{Decoding GRS Codes}
Having an encoded message is useless without someway to decode it. Reed-Solomon codes take advantage of a technique called \textit{syndrome decoding}. Syndrome decoding uses the dual code and check matrices to decode and to detect and correct errors that may have occurred in transmission. 

The code $\text{GRS}_{n,k}(\boldsymbol\alpha, \textbf{v})$ over $F$ is equal to the set of all $n$-tuples $\textbf{c} = (c_{1},c_{2},...,c_{n}) \in F^{n}$ such that:
\begin{center}
$\sum\limits_{i = 1}^{n} \dfrac{c_{i}u_{i}}{1 - \alpha_{i}z} = 0$ (mod $z^{r}$),
\end{center}
where $r = n - k \text{ and } u_{i}^{-1} = v_{i}\prod\limits_{j \neq i} (\alpha_{i} - \alpha_{j})$. This is known as the \textit{Goppa formulation for GRS codes}. The Goppa formulation is simply another way of writing our code, but there are benefits to using this formulation as we will see below.

If $\textbf{c} = (c_{1},c_{2},...,c_{n})$, a GRS code in the Goppa formulation, is transmitted, and $\textbf{p} = (p_{1},p_{2},...,p_{n})$ is received, then there is some vector $\textbf{e} = (e_{1},e_{2},...e_{n})$ such that $\textbf{p} = \textbf{c} + \textbf{e}$. This vector is called the error vector. We can then calculate the \textit{syndrome polynomial} of \textbf{p}:
\begin{center}
$S_{\textbf{p}}(z) = \sum\limits_{i=1}^{n} \dfrac{p_{i}u_{i}}{1 - \alpha_{i}^{z}}$ (mod $z^{r}$).
\end{center}
We can do the same for $\textbf{c}$ and $\textbf{e}$ as well. The following equation holds true:
\begin{center}
$S_{\textbf{p}}(z) = S_{\textbf{c}}(z) + S_{\textbf{e}}(z)$ (mod $z^{r}$).
\end{center}
Since $\textbf{c}$ is in the Goppa formulation
\begin{center}
$S_{\textbf{c}}(z) = \sum\limits_{i = 1}^{n} \dfrac{c_{i}u_{i}}{1 - \alpha_{i}z} = 0$,
\end{center}
so we write:
\begin{center}
$S_{\textbf{p}}(z) = S_{\textbf{e}}(z)$ (mod $z^{r}$).
\end{center}
Let $B$ be the set of error locations:
\begin{center}
$B =  \{i \mid e_{i} \neq 0\}$.
\end{center}
Then the syndrome polynomial reduces further to:
\begin{center}
$S_{\textbf{p}}(z) = S_{\textbf{e}}(z) = \sum\limits_{b \in B} \dfrac{e_{b}u_{b}}{1 - \alpha_{b}z} $ (mod $z^{r}$).
\end{center}
We can drop the subscripts and just write $S(z)$ for the syndrome polynomial.

The next step is to find the \textit{Key Equation}. This is done by clearing the denominators:
\begin{center}
$\sigma(z)S(z) = \omega(z)$ (mod $z^{r}$),
\end{center}
where
\begin{center}
$\sigma(z) = \sigma_{\textbf{e}}(z) = \prod\limits_{b \in B} (1 - \alpha_{b}z)$
\end{center}
and
\begin{center}
$\omega(z) = \omega{\textbf{e}}(z) = \sum\limits_{b \in B} e_{b}u_{b}\left(\prod\limits_{a \in B, a \neq b} (1 - \alpha_{a}z)\right)$.
\end{center}
The polynomial $\sigma(z)$ is called the \textit{error locator}, and the polynomial $\omega(z)$ is called the \textit{error evaluator}.

We can determine the error vector $\textbf{e}$ given $\sigma(z) \text{ and } \omega(z)$. If we assume for now that none of the $a_{i}$ equal 0, then:
\begin{center}
$B = {b \mid \sigma(a_{b}^{-1}) = 0}$
\end{center}
and, for each $b \in B$,
\begin{center}
$e_{b} = \dfrac{-\alpha_{b}\omega(\alpha_{b}^{-1})}{u_{b}\sigma'(\alpha_{b}^{-1})}$,
\end{center}
where $\sigma'(z)$ is the derivative of $\sigma(z)$. It can be shown that the polynomials $\sigma(z)$ and $\omega(z)$ determine the error vector even when some $\alpha_{i}$ is 0.

This method of decoding requires us to solve the Key Equation in order to find the error vector. The following theorem gives us a characterization of $\sigma(z)$ and $\omega(z)$, helping us solve the Key Equation.

\textit{Given $r$ and $S(z) \in F[z]$ there is at most one pair of polynomials $\sigma(z),\omega(z) \in F[z]$ satisfying}:
\begin{enumerate}
	\item $\sigma(z)S(z) = \omega(z)$ (mod $z^{r}$);
	\item deg$(\sigma(z)) \le r/2$ and deg$(\omega(z)) < r/2$;
	\item gcd$(\sigma(z),\omega(z)) = 1$ and $\sigma(0) = 1$.
\end{enumerate}

Using this characterization we can solve the Key Equation using the Euclidean algorithm. For a code $\text{GRS}_{n,k}(\boldsymbol\alpha, \textbf{v})$ over $F$, with $r = n - k$, given a syndrome polynomial $S(z)$, the following algorithm halts, producing $\Tilde{\sigma}(z)$ and $\Tilde{\omega}(z)$:
\begin{algorithm}[H]
	\caption{Decoding GRS using the Euclidean Algorithm\label{decoding}}
	Set $a(z) = z^{r}$ and $b(z) = S(z)$.
	
	Step through the Euclidean Algorithm until at Step $j$, deg$(r_{j}(z)) < r/2$.
	
	Set $\Tilde{\sigma}(z) = t_{j}(z)$ and $\Tilde{\omega}(z) = r_{j}(z)$.
\end{algorithm}
If the error vector exists, then $\Hat{\sigma}(z) = \Tilde{\sigma}(0)^{-1}\Tilde{\sigma}(z)$ and $\Hat{\omega}(z) = \Tilde{\sigma}(0)^{-1}\Tilde{\omega}(z)$ are the error locator and error evaluator polynomials for $\textbf{e}$. However, there are a few stipulations. The number of roots of $\Hat{\sigma}(z)$ among the $\alpha_{i}^{-1}$ must be equal to the degree of $\Hat{\sigma}(z)$. If this is not the case, that means we have detected errors which we cannot correct. Also, if $t_{j}(0) = 0$ we cannot perform the final division to determine $\Hat{\sigma}(z)$.

If $t_{j}(0) \neq 0$ and $\Hat{\sigma}(z)$ has the correct number of roots, then we can evaluate errors at each location to find a vector of weight at most $r/2$ with our original syndrome. At this point we have either decoded correctly, or we had more than $r/2$ errors and could not decode properly.

\section{Conclusion}
[Coming Soon]

\end{document}
